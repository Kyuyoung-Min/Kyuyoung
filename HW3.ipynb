{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트마이닝과 웹크롤링\n",
    "----\n",
    "\n",
    "# 1. 텍스트 마이닝의 이해\n",
    "### 1. 정의\n",
    "- 텍스트 마이닝이란? 텍스트로 양질의 정보를 뽑아내는 것.\n",
    "- 통계적인 패턴 학습을 통해 패턴과 트렌드를 찾는다. (대표적으로 회귀분석) \n",
    "- 비정형 데이터를 분석 가능한 정형 데이터로 바꾸는 것이 목표\n",
    "- 텍스트를 데이터로 변환 (자연어 처리방법)-> 패턴이나 트렌드를 찾는다-> 유용한 정보\n",
    "\n",
    "### 2. 단계\n",
    " #### 2.1. 문서\n",
    " - tokenize: 단어를 쪼개는 것\n",
    " - normalize: 변형된 단어를 원형으로 돌리는 것      \n",
    "   \n",
    "     \n",
    "     \n",
    " #### 2.2. 순서가 의미가 있는 단어들의 시퀀스\n",
    " - Fixed size vector without sequence info\n",
    " - Fixed size vector with sequence info\n",
    " - Series of Word Embedding with sequence info\n",
    "\n",
    "### 3. 적용분야\n",
    " - **Document classification**: Sentiment analysis, Classification  \n",
    " - **Document generation**: Q&A, Summarization, Translation  \n",
    " - **Keyword extraction**: Tagging/Annotation      \n",
    " - **Topic modeling**: LSA, LDA\n",
    " \n",
    "\n",
    "\n",
    "# 2. 텍스트 마이닝 방법론\n",
    "- 목적: document, sentence 등을 sparse vector로 변환 (word들의 sequence. 중복o 순서o)  \n",
    "  \n",
    "### 1. Tokenize\n",
    "- Document를 Sentence의 집합으로 분리\n",
    "- Sentence를 Word의 집합으로 분리\n",
    "- 의미 없는 문자 등을 걸러 냄\n",
    "\n",
    "### 2. Text Normalization\n",
    "- 동일한 의미의 단어가 다른 형태를 갖는 것을 보완\n",
    "- Stemming (어간 추출): 단어의 다양한 변형을 하나로 통일. 규칙에 의한 변환\n",
    "- Lemmatization (표제어 추출): 사전을 이용하여 단어의 원형을 추출. 품사 고려\n",
    "\n",
    "### 3. POS-tagging\n",
    "- 토큰화와 정규화 작업을 통해 나누어진 형태소에 대해 품사를 결정하여 할당\n",
    "- 품사 결정 위해 문맥을 파악해야 함  \n",
    "\n",
    "### 4. Chunking\n",
    "- Chunk: 말모듬.  주어와 동사가 없는 두 단어 이상의 집합인 구(phrase)를 의미\n",
    "- 주어진 텍스트에서 이와 같은 chunk를 찾는 과정. 각 형태소들을 서로 겹치지 않으면서 의미가 있는 구로 묶어나가는 과정  \n",
    "\n",
    "### 4. BOW (Bag of Words)\n",
    "- Vector Space Model: 단어가 쓰여진 순서는 무시, 한번 이상 나타난 단어들에 대해 유(1)/무(0) 로 문서를 표현\n",
    "- Count vector: 유/무 대신 단어가 문서에 나타난 횟수로 표현  \n",
    "\n",
    "### 5. TFIDF(Term Frequency - Inverse Document Frequency)\n",
    "- 단어의 count를 단어가 나타난 문서의 수로 나눠서 자주 등장하지 않는 단어의 weight를 올림\n",
    "- Count vector의 문제점 보완 \n",
    "\n",
    "\n",
    "# 3. 텍스트 마이닝 문제점\n",
    "### 1. 차원의 저주(Curse of Dimensionality)\n",
    "- Extremely sparse data\n",
    "- 각 데이터 간의 거리가 너무 멀게 위치.  \n",
    "\n",
    "### 2. 단어 빈도의 불균형\n",
    "- Zipf's law(멱법칙): 극히 소수의 데이터가 결정적인 영향을 미치게 됨  \n",
    "\n",
    "### 3. 단어가 쓰인 순서정보의 손실\n",
    "- 통계에 의한 의미 파악 \n",
    "- Loss of sequence information: context가 중요\n",
    "\n",
    "\n",
    "# 4. 텍스트 마이닝 해결방안 \n",
    "### 1. Feature Extraction\n",
    " #### 1.1. 주성분 분석(PCA)\n",
    " - 데이터의 분산을 최대한 보존하는 새로운 축을 찾아 변환함으로써 차원을 축소\n",
    " - 선형결합  \n",
    " \n",
    " \n",
    " #### 1.2. LSA(SVD)\n",
    " - 의미가 가까운 단어는 비슷한 곳에 위치\n",
    " - 주어진 dtm(document term matrix, A)을 A=UΣVT의 형태로 분해\n",
    " - 잠재의미분석: 문서 간의 유사도, 단어 간의 유사도\n",
    "\n",
    "### 2. Topic Modeling\n",
    "- 문서는 topic들로 구성. 각 토픽은 word들로 구성되며 그 가중치가 있다.\n",
    "\n",
    "\n",
    "### 3. Word Embedding\n",
    "> \"단어들을 어떻게 컴퓨터 안에서 표현하고 다룰까?\"  \n",
    "\n",
    "- one-hot-encoding: 각 단어를 모든 문서에서 사용된 단어들의 수 길이의 벡터로 표현. 자기 위치에만 (하나만) 1인 벡터로 인코딩. 길이가 매우 길어짐. \n",
    "- one-hot-enconding으로 표현된 단어를 dense vector로 변환. (길이가 어느정도인지 상관없이 일정한 길이로 인코딩해줌). \n",
    "- 최종목적에 맞게 **학습**에 의해 vector가 결정됨\n",
    "- document: 제한된 maxlen개의 word sequence\n",
    "- word: one-hot-vector에서 embedding된 dense vector \n",
    "\n",
    "#### 3.1.Word2Vec\n",
    " - 문장에 나타난 단어들의 순서를 이용해 word embedding 수행.\n",
    " - 단어의 위치에 기반하여 의미를 내포하는 vector 생성. (비슷한 위치에 나타나는 단어들은 비슷한 vector 가짐)\n",
    " - fixed embedding\n",
    " - CBOW: 주변단어들을 이용해 다음단어 예측\n",
    " - Skip-gram: 한 단어의 주변단어들을 예측 \n",
    "\n",
    "#### 3.2. ELMo\n",
    " - 동일한 단어가 문맥에 따라 전혀 다른 의미를 가지는 것을 반영하기 위해 개발된 워드 임베딩 기법.\n",
    " - biLSTM 사용. 정방향 문맥과 역방향 문맥을 다 사용. \n",
    "\n",
    "#### 3.3. Document Embedding\n",
    "- Word2Vec 모형 기반 + documet 고유 vector 학습 -> dense vctor 생성 -> 매칭, 분류 작업 수행 \n",
    "- fixed하고 denseg한 vector로 documet를 embedding 하고자함.\n",
    "\n",
    "### 4. RBM\n",
    "- 차원을 변경하면서 원래의 정보량을 최대한 유지하는 것이 목적\n",
    "\n",
    "### 5. Autoencoder\n",
    "- 차원을 축소하고 다시 복원했을때, Input 정보와 Output 정보가 최대한 동일하도록. \n",
    " \n",
    "\n",
    "### 6.N-gram\n",
    "- 문장을 한 단위로 쪼개지 않고 두개 이상(N개) 단위로 쪼개는 것 \n",
    "\n",
    "### 7.RNN\n",
    "- 뒷 단어에 대한 hidden node가 앞 단어의 hidden node 값에도 영향을 받도록 함(앞에서의 문맥 정보가 축적되어감)\n",
    "- 뒤로 갈수록 앞쪽의 정보는 소실된다는 문제점.\n",
    "\n",
    "### 8.LSTM\n",
    "- 직통 통로 만들어 RNN의 정보소실 문제 해결\n",
    "\n",
    "### 9.Bi-LSTM\n",
    "- 양방향으로 LSTM 구성하여 두 결과를 합침.\n",
    "\n",
    "### 10.CNN \n",
    "- 원래는 이미지 처리를 위해 개발되었으나, 주변 정보를 학습한다는 점을 이용하여 텍스트의 문맥을 학습. 문서 분류하는 연구 진행.\n",
    "\n",
    "### 11.Sequence-to-sequence\n",
    "- 입력과 출력 모두 sequence. 번역 위해 개발\n",
    "- 번역, chat-bot, summarize 등에 활용\n",
    "\n",
    "### 12.Attention \n",
    "- 입력의 단어들로부터 출력 단어에 **직접 링크**를 만듦\n",
    "- econder에 대해서 직접적으로 attention weights값을 구해 연결.\n",
    "\n",
    "### 13.Transformer(Self-attention)\n",
    "- ecnoder와 decoder\n",
    "- 입력 단어들끼리도 상호 연관성이 있는 것에서 착안\n",
    "- 입력->출력의 atteontion **+** 입력 단어들 간의 attention **+** 입력+출력->출력의 attention\n",
    "\n",
    "### 14.BERT\n",
    "- 양방향 transformer encoder 사용\n",
    "- segment, position embedding 사용\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# 실습코드 \n",
    "\n",
    "# 5. 웹 크롤링1 - Static Crawling\n",
    "\n",
    "## 1. urllib\n",
    "- 파이썬은 웹 사이트에 있는 데이터를 추출하기 위해 urllib 라이브러리 사용\n",
    "- 이를 이용해 HTTP 또는 FTP를 사용해 데이터 다운로드 가능\n",
    "- **urllib**은 URL을 다루는 모듈을 모아 놓은 패키지\n",
    "- **urllib.request** 모듈은 웹 사이트에 있는 데이터에 접근하는 기능 제공, 또한 인증, 다렉트, 쿠키처럼 인터넷을 이용한 다양한 요청과 처리가 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. urllib.request를 이용한 다운로드\n",
    "- urllib.request 모듈에 있는 urlretrieve() 함수 이용\n",
    "- 다음의 코드는 PNG 파일을 test.png 라는 이름의 파일로 저장하는 예제임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장되었습니다\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 읽어들이기 \n",
    "from urllib import request\n",
    "\n",
    "url=\"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename=\"test.png\" # 저장명\n",
    "\n",
    "request.urlretrieve(url, savename)\n",
    "print(\"저장되었습니다\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. urlopen으로 파일에 저장하는 방법\n",
    "- request.urlopen()은 메모리에 데이터를 올린 후 파일에 저장하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장되었습니다..\n"
     ]
    }
   ],
   "source": [
    "# URL과 저장경로 지정하기\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename = \"test1.png\" \n",
    "#다운로드\n",
    "mem = request.urlopen(url).read()\n",
    "#파일로 저장하기, wb는 쓰기와 바이너리모드\n",
    "with open(savename, mode=\"wb\") as f:\n",
    "    f.write(mem)\n",
    "    print(\"저장되었습니다..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. API 사용하기\n",
    "### 클라이언트 접속 정보 출력 (기본)\n",
    "- API는 사용자의 요청에 따라 정보를 반환하는 프로그램\n",
    "- IP 주소, UserAgent 등 클라이언트 접속정보 출력하는 \"IP 확인 API\" 접근해서 정보를 추출하는 프로그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ip]\n",
      "API_URI=http://api.aoikujira.com/ip/get.php\n",
      "REMOTE_ADDR=114.108.24.51\n",
      "REMOTE_HOST=114.108.24.51\n",
      "REMOTE_PORT=39294\n",
      "HTTP_HOST=api.aoikujira.com\n",
      "HTTP_USER_AGENT=Python-urllib/3.8\n",
      "HTTP_ACCEPT_LANGUAGE=\n",
      "HTTP_ACCEPT_CHARSET=\n",
      "SERVER_PORT=80\n",
      "FORMAT=ini\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#데이터 읽어들이기\n",
    "url=\"http://api.aoikujira.com/ip/ini\"\n",
    "res=request.urlopen(url)\n",
    "data=res.read()\n",
    "\n",
    "#바이너리를 문자열로 변환하기\n",
    "text=data.decode(\"utf-8\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BeautifulSoup\n",
    "- 스크레이핑(Scraping or Crawling)이란 웹 사이트에서 데이터를 추출하고, 원하는 정보를 추출하는 것을 의미\n",
    "- **BeautifulSoup**란 파이썬으로 스크레이핑할 때 사용되는 라이브러리로서 HTML/XML에서 정보를 추출할 수 있도록 도와줌. 그러나 다운로드 기능은 없음.\n",
    "- 파이썬 라이브러리는 pip 명령어를 이용해 설치 가능. Python Package Index(PyPI)에 있는 패키지 명령어를 한줄로 설치 가능\n",
    " - URL (http://pypi.python.org/pypi)\n",
    " \n",
    "## 패키지 import 및 예제 HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"\"\"\n",
    "<html><body>\n",
    "  <h1>스크레이핑이란?</h1>\n",
    "  <p>웹 페이지를 분석하는 것</p>\n",
    "  <p>원하는 부분을 추출하는 것</p>\n",
    "</body></html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. 기본 사용\n",
    "- 다음은 Beautifulsoup를 이용하여 웹사이트로부터 HTML을 가져와 문자열로 만들어 이용하는 예제임\n",
    "- h1 태그를 접근하기 위해 html-body-h1 구조를 사용하여 soup.html.body.h1 이런식으로 이용하게 됨.\n",
    "- p 태그는 두개가 있어 soup.html.body.p 한 후 next_sibling을 두번 이용하여 다음 p를 추출. 한번만 하면 그 다음 공백이 추출됨.\n",
    "- HTML 태그가 복잡한 경우 이런 방식으로 계속 진행하기는 적합하지 않음.\n",
    " ### 2) HTML 분석하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3) 원하는 부분 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = soup.html.body.h1\n",
    "p1 = soup.html.body.p\n",
    "p2 = p1.next_sibling.next_sibling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 4) 요소의 글자 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 = 스크레이핑이란?\n",
      "p  = 웹 페이지를 분석하는 것\n",
      "p  = 원하는 부분을 추출하는 것\n"
     ]
    }
   ],
   "source": [
    "print(f\"h1 = {h1.string}\")\n",
    "print(f\"p  = {p1.string}\")\n",
    "print(f\"p  = {p2.string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. 요소를 찾는 method\n",
    "\n",
    "### 단일 element 추출: find()\n",
    "BeautifulSoup는 루트부터 하나하나 요소를 찾는 방법 말고도 find()라는 메소드를 제공함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>스크레이핑이란?</h1>\n",
      "#title = 스크레이핑이란?\n",
      "#body = 웹 페이지를 분석하는 것\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# find() 메서드로 원하는 부분 추출\n",
    "title = soup.find(\"h1\")\n",
    "body  = soup.find(\"p\")\n",
    "print(title)\n",
    "\n",
    "# 텍스트 부분 출력\n",
    "print(f\"#title = {title.string}\" )\n",
    "print(f\"#body = {body.string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 복수 elements 추출: find_all()\n",
    " 여러개의 태그를 한번에 추출하고자 할때 사용함. 다음의 예제에서는 여러개의 태그를 추출하는 법을 보여주고 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"http://www.naver.com\">naver</a>, <a href=\"http://www.daum.net\">daum</a>] 2\n",
      "naver > http://www.naver.com\n",
      "daum > http://www.daum.net\n"
     ]
    }
   ],
   "source": [
    "html = \"\"\"\n",
    "<html><body>\n",
    "  <ul>\n",
    "    <li><a href=\"http://www.naver.com\">naver</a></li>\n",
    "    <li><a href=\"http://www.daum.net\">daum</a></li>\n",
    "  </ul>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# find_all() 메서드로 추출\n",
    "links = soup.find_all(\"a\")\n",
    "print(links, len(links))\n",
    "\n",
    "# 링크 목록 출력\n",
    "for a in links:\n",
    "    href = a.attrs['href'] # href의 속성에 있는 속성값을 추출\n",
    "    text = a.string \n",
    "    print(text, \">\", href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Css Selector\n",
    "> Css Selector란, 웹상의 요소에 css를 적용하기 위한 문법으로, 즉 요소를 선택하기 위한 패턴입니다.  \n",
    "출처: https://www.w3schools.com/cssref/css_selectors.asp  \n",
    "\n",
    "앞서 간단하게 태그를 사용하여 데이터를 추출하는 방법에 대해서 살펴보았습니다.\n",
    "\n",
    "하지만 복잡하게 구조화된 웹 사이트에서 자신이 원하는 데이터를 가져오기 위해서는 Css Selector에 대한 이해가 필요합니다.\n",
    "\n",
    "## BeautifulSoup에서 Css Selector 사용하기\n",
    "BeautifulSoup에서는 Css Selector로 값을 가져올 수 있도록 find와는 다른 메서드를 제공합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 = 위키북스 도서\n",
      "li = 유니티 게임 이펙트 입문\n",
      "li = 스위프트로 시작하는 아이폰 앱 개발 교과서\n",
      "li = 모던 웹사이트 디자인의 정석\n"
     ]
    }
   ],
   "source": [
    "html = \"\"\"\n",
    "<html><body>\n",
    "<div id=\"meigen\">\n",
    "  <h1>위키북스 도서</h1>\n",
    "  <ul class=\"items\">\n",
    "    <li>유니티 게임 이펙트 입문</li>\n",
    "    <li>스위프트로 시작하는 아이폰 앱 개발 교과서</li>\n",
    "    <li>모던 웹사이트 디자인의 정석</li>\n",
    "  </ul>\n",
    "</div>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML 분석하기 \n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 필요한 부분을 CSS 쿼리로 추출하기\n",
    "# 타이틀 부분 추출하기 --- (※3)\n",
    "h1 = soup.select_one(\"div#meigen > h1\").string # soup.select_one(선택자): CSS 선택자로 요소 하나를 추출\n",
    "print(f\"h1 = {h1}\")\n",
    "\n",
    "# 목록 부분 추출하기 --- (※4)\n",
    "li_list = soup.select(\"div#meigen > ul.items > li\") # soup.select(선택자): CSS 선택자로 요소 여러 개를 리스트를 추출\n",
    "for li in li_list:\n",
    "  print(f\"li = {li.string}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 활용 예제\n",
    "앞서 배운 **urllib**과 **BeautifulSoup**를 조합하면, 웹스크레이핑 및 API 요청 작업을 쉽게 수행하실 수 있습니다.\n",
    " 1. URL을 이용하여 웹으로부터 html을 읽어들임 (**urllib**)\n",
    " 2. html 분석 및 원하는 데이터를 추출 (**BeautifulSoup**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib import request, parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. 네이버 금융 - 환율 정보\n",
    "- 다양한 금융 정보가 공개돼 있는 \"네이버 금융\"에서 원/달러 환율 정보를 추출해보자!\n",
    "- 네이버 금융의 시장 지표 페이지 https://finance.naver.com/marketindex/\n",
    "- 다음은 원/달러 환율 정보를 추출하는 프로그램임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usd/krw = 1,175.00\n"
     ]
    }
   ],
   "source": [
    "# HTML 가져오기\n",
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "res = request.urlopen(url)\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# 원하는 데이터 추출\n",
    "price = soup.select_one(\"div.head_info > span.value\").string # CSS 선택자로 요소 하나 추출\n",
    "print(\"usd/krw =\", price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. 기상청 RSS\n",
    "- 기상청 RSS에서 특정 내용을 추출하는 예제\n",
    "- 기상청 RSS에서 XML 데이터를 추출하고 XML 내용을 출력\n",
    "- 기상청의 RSS 서비스에 지역 번호를 지정하여 데이터 요청해보기\n",
    "http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp\n",
    " - 참고: 기상청 RSS http://www.kma.go.kr/weather/lifenindustry/service_rss.jsp\n",
    "- 파이썬으로 요청 전용 매개변수를 만들 때는 **urllib.parse** 모듈의 **urlencode()** 함수를 사용해 매개변수를 URL로 인코딩한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url= http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=109\n",
      "서울,경기도 육상중기예보\n",
      "○ (강수) 1일(목) 오후~2일(금) 오전에는 비가 내리겠습니다.<br />○ (기온) 이번 예보기간 낮 기온은 20~25도로 오늘(25일, 24~27도)과 비슷하거나 조금 낮겠고, 아침 기온은 9~17도로 선선하겠습니다.<br />          특히, 내륙을 중심으로 낮과 밤의 기온차가 10도 내외로 크겠습니다.<br />○ (해상) 서해중부해상의 물결은 0.5~2.0m로 일겠습니다.\n",
      "서울,경기도 육상중기예보\n",
      "○ (강수) 1일(목) 오후~2일(금) 오전에는 비가 내리겠습니다.<br />○ (기온) 이번 예보기간 낮 기온은 20~25도로 오늘(25일, 24~27도)과 비슷하거나 조금 낮겠고, 아침 기온은 9~17도로 선선하겠습니다.<br />          특히, 내륙을 중심으로 낮과 밤의 기온차가 10도 내외로 크겠습니다.<br />○ (해상) 서해중부해상의 물결은 0.5~2.0m로 일겠습니다.\n"
     ]
    }
   ],
   "source": [
    "# HTML 가져오기\n",
    "url = \"http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp\"\n",
    "\n",
    "#매개변수를 URL로 인코딩한다.\n",
    "values = {\n",
    "    'stnId':'109'\n",
    "}\n",
    "\n",
    "params=parse.urlencode(values)\n",
    "url += \"?\"+params # URL에 매개변수 추가\n",
    "print(\"url=\", url)\n",
    "\n",
    "res = request.urlopen(url)\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# 원하는 데이터 추출\n",
    "header = soup.find(\"header\") \n",
    "\n",
    "title = header.find(\"title\").text\n",
    "wf = header.find(\"wf\").text\n",
    "\n",
    "print(title)\n",
    "print(wf)\n",
    "\n",
    "# CSS selector 기반\n",
    "title = soup.select_one(\"header > title\").text\n",
    "wf = header.select_one(\"header wf\").text\n",
    "\n",
    "print(title)\n",
    "print(wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. 윤동주 작가의 작품 목록\n",
    "- 위키문헌 (https://ko.wikisource.org/wiki) 에 공개되어 있는 윤동주의 작품목록을 가져오기\n",
    "- 윤동주 위키 (https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC)\n",
    "- 하늘과 바람과 시 부분을 선택한 후 오른쪽 마우스 이용해 copy selector로 카피하면 다음의 CSS 선택자가 카피됨\n",
    " - #mw-content-text > div > ul:nth-child(6) > li > b > a\n",
    "- nth-child(n) 은 n 번째 요소를 의미 즉 6번째 요소를 의미, #mw-content-text 내부에 있는 url 태그는 모두 작품과 관련된 태그. 따라서 따로 구분할 필요는 없으며 생략해도 됨. BeautifulSoup는 nth-child 지원하지 않음\n",
    " - Recall PR7 Problem1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 하늘과 바람과 별과 시\n",
      "- 증보판\n",
      "- 서시\n",
      "- 자화상\n",
      "- 소년\n",
      "- 눈 오는 지도\n",
      "- 돌아와 보는 밤\n",
      "- 병원\n",
      "- 새로운 길\n",
      "- 간판 없는 거리\n",
      "- 태초의 아침\n",
      "- 또 태초의 아침\n",
      "- 새벽이 올 때까지\n",
      "- 무서운 시간\n",
      "- 십자가\n",
      "- 바람이 불어\n",
      "- 슬픈 족속\n",
      "- 눈감고 간다\n",
      "- 또 다른 고향\n",
      "- 길\n",
      "- 별 헤는 밤\n",
      "- 흰 그림자\n",
      "- 사랑스런 추억\n",
      "- 흐르는 거리\n",
      "- 쉽게 씌어진 시\n",
      "- 봄\n",
      "- 참회록\n",
      "- 간(肝)\n",
      "- 위로\n",
      "- 팔복\n",
      "- 못자는밤\n",
      "- 달같이\n",
      "- 고추밭\n",
      "- 아우의 인상화\n",
      "- 사랑의 전당\n",
      "- 이적\n",
      "- 비오는 밤\n",
      "- 산골물\n",
      "- 유언\n",
      "- 창\n",
      "- 바다\n",
      "- 비로봉\n",
      "- 산협의 오후\n",
      "- 명상\n",
      "- 소낙비\n",
      "- 한난계\n",
      "- 풍경\n",
      "- 달밤\n",
      "- 장\n",
      "- 밤\n",
      "- 황혼이 바다가 되어\n",
      "- 아침\n",
      "- 빨래\n",
      "- 꿈은 깨어지고\n",
      "- 산림\n",
      "- 이런날\n",
      "- 산상\n",
      "- 양지쪽\n",
      "- 닭\n",
      "- 가슴 1\n",
      "- 가슴 2\n",
      "- 비둘기\n",
      "- 황혼\n",
      "- 남쪽 하늘\n",
      "- 창공\n",
      "- 거리에서\n",
      "- 삶과 죽음\n",
      "- 초한대\n",
      "- 산울림\n",
      "- 해바라기 얼굴\n",
      "- 귀뚜라미와 나와\n",
      "- 애기의 새벽\n",
      "- 햇빛·바람\n",
      "- 반디불\n",
      "- 둘 다\n",
      "- 거짓부리\n",
      "- 눈\n",
      "- 참새\n",
      "- 버선본\n",
      "- 편지\n",
      "- 봄\n",
      "- 무얼 먹구 사나\n",
      "- 굴뚝\n",
      "- 햇비\n",
      "- 빗자루\n",
      "- 기왓장 내외\n",
      "- 오줌싸개 지도\n",
      "- 병아리\n",
      "- 조개껍질\n",
      "- 겨울\n",
      "- 트루게네프의 언덕\n",
      "- 달을 쏘다\n",
      "- 별똥 떨어진 데\n",
      "- 화원에 꽃이 핀다\n",
      "- 종시\n"
     ]
    }
   ],
   "source": [
    "url = \"https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC\"\n",
    "res = request.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# #mw-content-text 바로 아래에 있는 \n",
    "# ul 태그 바로 아래에 있는\n",
    "# li 태그 아래에 있는\n",
    "# a 태그를 모두 선택합니다.\n",
    "a_list = soup.select(\"#mw-content-text   ul > li  a\")\n",
    "for a in a_list:\n",
    "    name = a.string\n",
    "    print(f\"- {name}\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 일반문제\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 네이버 뉴스 헤드라인\n",
    "배운 내용을 바탕으로 네이버 뉴스(https://news.naver.com/)에서 헤드라인 뉴스의 제목을 추출해보고자 합니다.\n",
    "> Q: 다음의 코드에 css selector를 추가하여 최신 기사의 헤드라인을 스크레이핑하는 코드를 완성하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "재난지원금 200만 원이라더니..'오락가락 행정'에 또 한 번 '분통'\n",
      "日스가, 中시진핑과 전화회담..시진핑 방일 논의 없었다\n",
      "법무부 \"조두순 재범위험성 있다\"..음주제한 등 청구 방침\n",
      "피격 하루 만에 김정은 사과..북한의 의도는?\n",
      "윤건영 \"목함지뢰 때 朴은 강강술래..野, 아카펠라 공연 운운 자격 있나\"\n",
      "푸틴 \"韓과 유익한 협력경험 축적..상호호혜 관계 계속 발전\"\n",
      "도쿄올림픽 '입촌식' 없다..선수단도 10-15% 감축\n",
      "\"진접선 개통, 내년 말로 연기\"..남양주시장 입장문 발표\n",
      "트럼프 잇따른 '대선 불복' 시사에 美 민주주의 뿌리째 흔들린다\n",
      "푸틴, 미국에 '선거·내정 등 간섭 금지 협약 맺자' 제안\n",
      "민간인 속에 숨은 알카에다..미군은 '닌자 미사일'로 제거했다\n",
      "165Hz, 커브드 지원 본격 게이밍 모니터, 루컴즈 스펙트럼 M3202DQ\n"
     ]
    }
   ],
   "source": [
    "## NAVER 뉴스가 오류나서 DAUM 으로 대체. \n",
    "url = \"https://news.daum.net/\" # html 가져오기\n",
    "\n",
    "res = request.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\") # html 분석\n",
    "\n",
    "selector = \"#mArticle > div.box_headline > ul > li > strong.tit_g > a\" # selector 설정\n",
    "\n",
    "for a in soup.select(selector):  # CSS 선택자로 요소 여러개를 추출 \n",
    "    title = a.text\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#url = \"https://news.naver.com/\" # html 가져오기\n",
    "\n",
    "#res = request.urlopen(url)\n",
    "#soup = BeautifulSoup(res, \"html.parser\") # html 분석\n",
    "\n",
    "#selector = \"#today_main_news > div.hdline_news > ul > li > div.hdline_article_tit > a\" # selector 설정\n",
    "\n",
    "#for a in soup.select(selector):  # CSS 선택자로 요소 여러개를 추출 \n",
    "    #title = a.text\n",
    "    #print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 시민의 소리 게시판\n",
    "다음은 서울시 대공원의 시민의 소리 게시판 입니다.\n",
    "https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgList.do?pgno=1\n",
    "\n",
    "해당 페이지에 나타난 게시글들의 제목을 수집하고자 합니다.\n",
    "> Q: 다음의 코드에 css selector를 추가하여 해당 페이지에서 게시글의 제목을 스크레이핑하는 코드를 완성하시오. 또한 과제 제출시 하단의 **추가 내용**을 참고하여 수집한 데이터를 csv 형태로 저장하여 해당 csv 파일도 함께 제출하시오.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['관리인 마스크', '어린이 대공원 쓰레기집하장 내 쓰레기 제거 요청 ', '마스크미착용으로 축구 및, 베트민턴 치는 인원이 너무 많아요.', '공원 내 마스크 착용', '청춘핫도그 점장님과 직원분께 감사드립니다', '카드결제를 거부하는 매점을 신고합니다', '참얼굴만큼예쁘고맘씨좋은 여직원을 만나 고마워서 글을남깁니다.', '놀이동산에서 불쾌함을 겪었습니다', '서문 플래카드', '간만에 친절한 아가씨를 만났어요.(놀이동산)'] ['https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=yq3mQv5mfa1zvLB2Fr1blDb7ydxdLZCkLdwA2g6dJwv9JzrHVMj1SR6PUx5Fso8z.etisw1_servlet_user?qnaid=QNAS20200917000010&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=yq3mQv5mfa1zvLB2Fr1blDb7ydxdLZCkLdwA2g6dJwv9JzrHVMj1SR6PUx5Fso8z.etisw1_servlet_user?qnaid=QNAS20200902000003&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=yq3mQv5mfa1zvLB2Fr1blDb7ydxdLZCkLdwA2g6dJwv9JzrHVMj1SR6PUx5Fso8z.etisw1_servlet_user?qnaid=QNAS20200826000002&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=yq3mQv5mfa1zvLB2Fr1blDb7ydxdLZCkLdwA2g6dJwv9JzrHVMj1SR6PUx5Fso8z.etisw1_servlet_user?qnaid=QNAS20200825000003&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=yq3mQv5mfa1zvLB2Fr1blDb7ydxdLZCkLdwA2g6dJwv9JzrHVMj1SR6PUx5Fso8z.etisw1_servlet_user?qnaid=QNAS20200818000009&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=yq3mQv5mfa1zvLB2Fr1blDb7ydxdLZCkLdwA2g6dJwv9JzrHVMj1SR6PUx5Fso8z.etisw1_servlet_user?qnaid=QNAS20200816000002&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=yq3mQv5mfa1zvLB2Fr1blDb7ydxdLZCkLdwA2g6dJwv9JzrHVMj1SR6PUx5Fso8z.etisw1_servlet_user?qnaid=QNAS20200813000003&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=yq3mQv5mfa1zvLB2Fr1blDb7ydxdLZCkLdwA2g6dJwv9JzrHVMj1SR6PUx5Fso8z.etisw1_servlet_user?qnaid=QNAS20200813000002&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=yq3mQv5mfa1zvLB2Fr1blDb7ydxdLZCkLdwA2g6dJwv9JzrHVMj1SR6PUx5Fso8z.etisw1_servlet_user?qnaid=QNAS20200730000004&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=yq3mQv5mfa1zvLB2Fr1blDb7ydxdLZCkLdwA2g6dJwv9JzrHVMj1SR6PUx5Fso8z.etisw1_servlet_user?qnaid=QNAS20200728000002&pgno=1']\n"
     ]
    }
   ],
   "source": [
    "url_head = \"https://www.sisul.or.kr\"\n",
    "\n",
    "url_board = url_head + \"/open_content/childrenpark/qna/qnaMsgList.do?pgno=1\"\n",
    "\n",
    "\n",
    "\n",
    "res = request.urlopen(url_board)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "\n",
    "selector = \"#detail_con > div.generalboard > table > tbody > tr > td.left.title > a\"\n",
    "titles = []\n",
    "links = []\n",
    "for a in soup.select(selector):  # CSS 선택자로 요소 여러개를 추출\n",
    "    titles.append(a.text)\n",
    "    links.append(url_head + a.attrs[\"href\"])\n",
    "    \n",
    "print(titles, links)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추가 내용\n",
    "수집된 자료를 데이터프레임으로 만들어 csv로 저장하는 것이 일반적입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>관리인 마스크</td>\n",
       "      <td>https://www.sisul.or.kr/open_content/childrenp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>어린이 대공원 쓰레기집하장 내 쓰레기 제거 요청</td>\n",
       "      <td>https://www.sisul.or.kr/open_content/childrenp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>마스크미착용으로 축구 및, 베트민턴 치는 인원이 너무 많아요.</td>\n",
       "      <td>https://www.sisul.or.kr/open_content/childrenp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>공원 내 마스크 착용</td>\n",
       "      <td>https://www.sisul.or.kr/open_content/childrenp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>청춘핫도그 점장님과 직원분께 감사드립니다</td>\n",
       "      <td>https://www.sisul.or.kr/open_content/childrenp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                title  \\\n",
       "0                             관리인 마스크   \n",
       "1         어린이 대공원 쓰레기집하장 내 쓰레기 제거 요청    \n",
       "2  마스크미착용으로 축구 및, 베트민턴 치는 인원이 너무 많아요.   \n",
       "3                         공원 내 마스크 착용   \n",
       "4              청춘핫도그 점장님과 직원분께 감사드립니다   \n",
       "\n",
       "                                                link  \n",
       "0  https://www.sisul.or.kr/open_content/childrenp...  \n",
       "1  https://www.sisul.or.kr/open_content/childrenp...  \n",
       "2  https://www.sisul.or.kr/open_content/childrenp...  \n",
       "3  https://www.sisul.or.kr/open_content/childrenp...  \n",
       "4  https://www.sisul.or.kr/open_content/childrenp...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "board_df = pd.DataFrame({\"title\": titles, \"link\": links}) # 데이터프레임으로\n",
    "board_df.head() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
